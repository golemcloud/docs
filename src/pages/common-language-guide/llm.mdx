import { Tabs } from "nextra/components"
import { Steps } from "nextra/components"

# Talking to Large Language Models

Golem provides a [library called golem-llm](https://github.com/golemcloud/golem-llm) that is a WebAssembly Component allowing Golem components written in any of the supported languages to use a set of supported LLM providers.

The current list of supported LLMs are the following:

- [Anthropic (Claude)](https://www.anthropic.com)
- [xAI (Grok)](https://x.ai)
- [OpenAI](https://openai.com)
- [OpenRouter](https://openrouter.ai)

The library contains a [WIT interface](https://github.com/golemcloud/golem-llm/blob/main/wit/golem-llm.wit) providing a unified interface for all these providers.

## Starting with the LLM templates

There are _example templates_ provided by `golem component new` for some languages, called `example-gitpulse-agent`. To start from this example from scratch, use the following sequence of `golem` CLI commands:

<Tabs items={["Python"]}>
  <Tabs.Tab>
```shell copy
golem app new my-agent python
cd my-agent
golem component new python:example-gitpulse-agent
```
  </Tabs.Tab>
</Tabs>

## Adding LLM support to an existing component

To add the `golem-llm` dependency to an existing Golem component, follow following steps:

<Steps>
### Select one of the released WASM binaries

Go to the [releases page of golem-llm](https://github.com/golemcloud/golem-llm/releases) and copy the link of to the latest WASM binary of the desired LLM provider. The `-portable` versions are to be used _outside of Golem_ so choose the non-portable ones.

### Add a new dependency to `golem.yaml`

In the `golem.yaml` of the component, add the following new dependency:

```yaml
dependencies:
  my:component:  # <-- the name of your component you add the golem-llm dependency to
    - type: wasm
      url: https://github.com/golemcloud/golem-llm/releases/download/v0.1.2/golem_llm_openai.wasm
```

### Import golem-llm in WIT

Add the following `import` statement to your component's WIT file:

```wit copy
import golem:llm/llm@1.0.0;
```

### Rebuild the application

The next build will download the WASM component and regenerate the language-specific bindings for the new import:

```shell copy
golem app build
```

</Steps>

## Using the library

There are three top level functions exported from `golem-llm`:

```wit
send: func(messages: list<message>, config: config) -> chat-event;
continue: func(messages: list<message>, tool-results: list<tuple<tool-call, tool-result>>, config: config) -> chat-even;
%stream: func(messages: list<message>, config: config) -> chat-stream;
```

- `send` sends a prompt with a configuration to the LLM and returns the whole response
- `continue` can be used after one `send` and zero or more `continue` calls, if the returned result contained a tool call. After performing the call, the results can be passed back to the LLM with the `tool-results` parameter.
- `stream` returns a stream resource which can be used to process the LLM response incrementally as it arrives

For general information about how to construct the prompts and how tool usage works, read the chosen LLMs official documentation.

Images are supported in the input as messages referring to external images via an url.

### Simple text chat example

The following example demonstrates the usage of `send`:

<Tabs items={["Rust", "TypeScript", "Go", "Python", "C", "JavaScript", "Zig", "MoonBit", "Scala.js"]}>
  <Tabs.Tab>
  ```rust
    use crate::bindings::golem::llm::llm;

    let config = llm::Config {
            model: "gpt-3.5-turbo",
            temperature: Some(0.2),
            max_tokens: None,
            stop_sequences: None,
            tools: vec![],
            tool_choice: None,
            provider_options: vec![],
    };

    let response = llm::send(
        &[llm::Message {
            role: llm::Role::User,
            name: Some("user-name".to_string()),
            content: vec![llm::ContentPart::Text(
                "What is the usual weather on the Vršič pass in the beginning of May?".to_string(),
            )],
        }],
        &config,
    );
  ```
  </Tabs.Tab>
  <Tabs.Tab>
  ```typescript
  // TODO: Code example is not available yet
  ```
  </Tabs.Tab>
  <Tabs.Tab>
  ```go
  import (
  "app/components-go/my-component/binding/golem/llm/llm"
  )

 	config := llm.Config{
		Model:           "gpt-3.5-turbo",
		Temperature:     cm.Some[float32](0.2),
		MaxTokens:       cm.None[uint32](),
		StopSequences:   cm.None[cm.List[string]](),
		Tools:           cm.ToList([]llm.ToolDefinition{}),
		ToolChoice:      cm.None[string](),
		ProviderOptions: cm.ToList([]llm.Kv{}),
	}

	response := llm.Send(cm.ToList([]llm.Message{
		llm.Message{
			Role: llm.RoleUser,
			Name: cm.Some[string]("user-name"),
			Content: cm.ToList([]llm.ContentPart { llm.ContentPartText("What is the usual weather on the Vršič pass in the beginning of May?")})
		}
	}), config)
  ```
  </Tabs.Tab>
  <Tabs.Tab>
  ```python
  from my_component.imports import llm

  llm_config = llm.Config(
      model="gpt-3.5-turbo",
      temperature=0.2,
      tools=[],
      provider_options=[],
      max_tokens=None,
      tool_choice=None,
      stop_sequences=None,
  )

  result = llm.send(
        [
            llm.Message(
                role=llm.Role.SYSTEM,
                name=None,
                content=[
                    llm.ContentPart_Text(
                        f"""What is the usual weather on the Vršič pass in the beginning of May?"""
                    )
                ],
            )
        ],
        llm_config,
    )
  ```
  </Tabs.Tab>
  <Tabs.Tab>
  ```c
  // TODO: Code example is not available yet
  ```
  </Tabs.Tab>
  <Tabs.Tab>
  ```javascript
  // TODO: Code example is not available yet
  ```
  </Tabs.Tab>
  <Tabs.Tab>
  ```zig
  // TODO: Code example is not available yet
  ```
  </Tabs.Tab>
  <Tabs.Tab>
  ```moonbit
  // TODO: Code example is not available yet
  ```
  </Tabs.Tab>
  <Tabs.Tab>
  ```scala.js
  // TODO: Code example is not available yet
  ```
  </Tabs.Tab>
</Tabs>
